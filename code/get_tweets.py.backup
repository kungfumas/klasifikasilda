import tweepy
import json
import pandas as pd
import re
from pymongo import MongoClient


usa_states_regex = '(?:(A[KLRZ]|C[AOT]|D[CE]|FL|GA|HI|I[ADLN]|K[SY]|LA|M[ADEINOST]|N[CDEHJMVY]|O[HKR]|P[AR]|RI|S[CD]|T[NX]|UT|V[AIT]|W[AIVY]|USA))'

usa_states_fullname_regex = '(ALABAMA|ALASKA|ARIZONA|ARKANSAS|CALIFORNIA|COLORADO|CONNECTICUT|DELAWARE|FLORIDA|GEORGIA|HAWAII|' \
                            'IDAHO|ILLINOIS|INDIANA|IOWA|KANSAS|KENTUCKY|LOUISIANA|MAINE|MARYLAND|MASSACHUSETTS|MICHIGAN|MINNESOTA|MISSISSIPPI|MISSOURI|MONTANA|'\
                            'NEBRASKA|NEVADA|NEW\sHAMPSHIRE|NEWSJERSEY|NEW\sMEXICO|NEW\sYORK|NORTH\sCAROLINA|NORTH\sDAKOTA|OHIO|OKLAHOMA|OREGON|PENNSYLVANIA|RHODE\sISLAND|'\
                            'SOUTH\sCAROLINA|SOUTH\sDAKOTA|TENNESSEE|TEXAS|UTAH|VERMONT|VIRGINIA|WASHINGTON|WEST\sVIRGINIA|WISCONSIN|WYOMING)'

query_terms = '(covid OR covid19 or covid-19) AND ' \
        '(k12 OR k-12) AND ' \
        '(remote OR distance OR online OR virtual OR hybrid)' \

query_operators = ' -filter:retweets'

query_with_ops = query_terms + query_operators


def get_mongo_client(host='localhost', port=27017):
    mongo_client = MongoClient(host=host, port=port)
    return mongo_client

def get_twitter_collection(mongo_client):
    capstone_db = mongo_client['capstone_db']
    twitter_collection = capstone_db['tweets']
    return twitter_collection

def get_twitter_api(filepath):
    with open(filepath) as f:
        d = json.load(f)
        # OAuth 2 authentication for making API requests without the user context
        # Getting read-only access to public information
        auth = tweepy.OAuthHandler(consumer_key=d['consumer_key'], 
                                   consumer_secret=d['consumer_secret'])
        auth.set_access_token(d['access_token'], d['access_token_secret'])
    # prep for streaming
    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
    return api

def get_tweets(api, query=query_with_ops):
    # search twitter for #remotelearning -returns response in json format
    tweets = []
    for tweet in tweepy.Cursor(api.search,tweet_mode='extended', q=query, lang='en', include_entities='false', result_type='mixed', rpp=100).items(100):
        user_loc_str = tr(tweet.user.location).upper()
        found_usa_state = re.search(usa_states_regex, user_loc_str) or re.search(usa_states_fullname_regex, user_loc_str)
        if found_usa_state:
            if hasattr(tweet, 'retweeted_status'):
                print('Not storing Retweets')
            else:
                print(f'text:{full_text}, screen name:{screen_name}, loc: {user_loc_str} , {id_str}, {created_at}, {user_id_str} \n')
                tweets.append(tweet._json)

            '''\
            The tweet attributes we are most interested in using for the data cleaning, sentiment labeling and data analysis are:
            id_str = tweet.id_str
            created_at = tweet.created_at  # UTC time when this Tweet was created, ex: "Wed Oct 10 20:19:24 +0000 2018"
            full_text = tweet.full_text
            user_loc_str = tweet.user.location
            user_id_str = tweet.user.id_str 
            screen_name = tweet.user.screen_name (MAYBE just for CLEANING)
            '''
    return tweets


def export_mongo_tweets_to_dataframe(twitter_collection):
    df = pd.DataFrame(list(twitter_collection.find({})))  #select * from tweet_collection
    print(df.info())
    df.to_csv('../data/collected_tweets.csv')


if __name__=="__main__":
    fp = '/home/user/.ssh/twitter_app_capstone.json'

    mongo_client = get_mongo_client()
    twitter_collection = get_twitter_collection(mongo_client=mongo_client)
    original_count = twitter_collection.count_documents({})

    api = get_twitter_api(filepath=fp)
    tweets = get_tweets(api=api)


    twitter_collection.insert_many(tweets)
    new_count = twitter_collection.count_documents({})
    print(f"started with {original_count}, ended with {new_count}, added {new_count-original_count}")